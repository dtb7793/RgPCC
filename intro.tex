\documentclass[main.tex]{subfiles}

\begin{document}
\section{Introduction}
Principal component analysis (PCA) is a prominent unsupervised tool used in the analysis of high dimensional data. This method, as described in \cite{PCA}, changes the basis of the data to that of the eigenbasis of the sample covariance matrix. These eigenvectors are called \emph{principal components}. After this change of basis, we may perform dimension reduction by prioritizing the principal components with high variance (or large eigenvalues). In other words, we project the data to a low dimensional linear subspace spanned by a subset of the eigenbasis. This dimension reduction is then typically followed with a supervised task such as regression or classification.

Principal component regression (PCR) was introduced (by Jeffers, 1967) to deal with multicollinearity. This method performms ordinary least square regression on the top $K$ principal components. This $K$ is typically picked so that the top $K$ principal components account for 90\% of the variance. Alternatively, $K$ can be tuned with information criterion or crossvalidation. We can also accomplish classification tasks by following PCA with logistic regression (principal component logistic regression). While these methods have found success with respect to overtuning and dimension reduction, their downfall is that the principal components depend solely on the design $\X$ and in this sense the variable selection is ``blind", as it does not take the response into account.


In early 2020 Lang and Zou \cite{langzou} introduced Response-guided Principal Component Regression (RgPCR) to remedy the ``blind" selection of PCR. This is done by replacing the hard-thresholding of PCR with soft-thresholding via a penalty function. The result is that both the variance of the predictors and the association with the response of principal components is taken into account during thresholding. This achieves dimension reduction and regression simultaneously as well a regularization to prevent overfitting.


In this paper, we will modify RgPCR to be used for binary classification. To do this we replace the penalized least squares error of RgPCR with a penalized log likelihood function. We approximate the optimal solution by quadratically approximating the log-likelihood function in a way that can be interpretted as a RgPCR problem on weighted ``psuedo data". As in RgPCR, for the lasso penalty we have a closed form solution to this problem. By taking iterative quadratic approximations of the above type, we can approximate the of the penalized log likelihood function. The result is a principal component classification algorithm that takes the response into account during variable selection.
%In this paper, we will combine RgPCR with logistic regression for binary classification. To do this we optimized a penalized log likelihood function by quadratically approximating and taking advantage of the principal components of ``psuedo data" in the resulting expression. The result is a principal component classification algorithm that takes the response into account during variable selection.

In section 2 we will give our motivation and cover the background knowledge necessary. In section 3 we will derive the RgPCC problem and give an algorithm for when the penalty is LASSO. In sections 4 we will compare the performance of RgPCC against other methods on simulated and realworld data. Lastly, in section 5 we will summarize our conclusions and propose further topics of study. Section 6 is left for figures and tables.

\end{document}
