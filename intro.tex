\documentclass[main.tex]{subfiles}

\begin{document}
\section{Introduction}
Principal component analysis (PCA) is a prominent unsupervised tool used in the analysis of high dimensional data. This method changes the basis of the data to that of the eigenbasis of the sample covariance matrix. These eigenvectors are called \emph{principal components}. After this change of basis, we may perform dimension reduction by prioritizing the principal components with high variance (or large eigenvalues). In other words, we project the data to a low dimensional linear subspace spanned by a subset of the eigenbasis. This dimension reduction is then typically followed with a supervised task such as regression or classification. 

Principal component regression (PCR) was introduced (by Jeffers, 1967) to deal with multicollinearity. This method can achieve dimension reduction and improve prediction performance compared to ordinary least squares by regressing on the top $K$ principal components. However, its downfall in regression is that the principal components depend soley on the design $\X$ and in this sense the variable selection is ``blind", as it does not take the response into account.


In early 2020 Lang and Zou \cite{langzou} introduced Response-guided Principal Component Regression (RgPCR) to remedy the ``blind" selection of PCR. This is done by replacing the hard-thresholding of PCR with soft-thresholding via a penalty function. The result is that both the variance of the predictors and the association with the response of principal components is taken into account during thresholding.


In this paper, we will modify RgPCR to be used for binary classification. To do this we optimized a penalized log likelihood function by quadratically approximating and taking advantage of the principal components of ``psuedo data" in the resulting expression. The result is a principal component classification algorithm that takes the response into account during variable selection.
%In this paper, we will combine RgPCR with logistic regression for binary classification. To do this we optimized a penalized log likelihood function by quadratically approximating and taking advantage of the principal components of ``psuedo data" in the resulting expression. The result is a principal component classification algorithm that takes the response into account during variable selection.

In section 2 we will give our motivation and cover the background knowledge necessary. In section 3 we will derive the RgPCC algorithm. In sections 4 we will compare the performance of RgPCC against other methods on simulated and realworld data. Lastly, in section 5 we will summarize our conclusions and propose further topics of study.

\end{document}
