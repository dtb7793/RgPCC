\documentclass[main.tex]{subfiles}

\begin{document}
\section{Literature Review}
RgPCC is a combination of several existing techniques of machine learning. This method improves upon methods of PCA like PCR, uses the likelihood frame works of logistic regression and is ultimately a modification of RgPCR. Here we describe the following methods that have motivate RgPCC. In section 2.1 and 2.2 we summarize PCA and logistic regression respectively. In section 2.3 we motivate and outline RgPCR, a method by Lang and Zou \citep{langzou}.

%
%
%
%
%
%
%
%
%
%
%

\subsection{Principal Component Analysis (PCA)}
The central idea of PCA is to reduce the dimensionality of data given that there is some colinearity between the variables. Our goal is to retain as much of the variance as possible while reducing the dimensionality. Intuitively, we are looking for a linear function $\alpha_1$ such that $\text{var}(\alpha_1^T \X)$ is maximized. This $\alpha_1$ is then called the \emph{first principal component}, the direction in which the data $\X$ varies the most. We can then find a direction $\alpha_2$ orthogonal to $\alpha_1$ that maximizes $\text{var}(\alpha^T_2 \X)$ for the second principal component and so on. 

To simplify the above computation, the above can be recognized as a generalized eigenvalue problem $\max \alpha_1^T \Sigma \alpha_1$ where $\Sigma = \X^T \X$. Thus we can simply compute the spectrum of $\Sigma$ and or the singular value decomposition of $\X$. Thus, principal components make up the eigenbasis of $\Sigma$ which are the columns of $\V$ in $\X = \U \D \V^T$. We thus work with the definition that the $k^{\text{th}}$ principal component of $\X$ is the eigenvector $v_k$ of $\Sigma$ associated to the $k^{\text{th}}$ largest eigenvalue. In particular, when $\D$ is written from largest to smallest singular values, then the $k^{\text{th}}$ principal component is simply the $k^{\text{th}}$ column of $\V$. More information on this definition can be found in \cite{PCA}.

In practice, we wish to use our data projected down to the linear subspace spanned by the first $K$ principal components. This is computed by rearranging the SVD as $\X \V = \U \D$. In some cases, we may wish to work with $\U$ itself which can be interpretted as a normalized version of the projected data.

%
%
%
%
%
%
%
%
%
%
%

\subsection{Response-guided Principal Component Regression (RgPCR)}
RgPCR is a generalization of ridge regression which we will briefly summarize. Let $\X = \U \D \V^T$ be the singular value decomposition of an $N \times p$ design matrix $\X$. Then with response $\y$ we can write the ridge regression solution as
\begin{align}
    \gls{betahat}^{\ridge} = \argmin_{\bbeta} ||\y - \X \gls{beta}||_2^2 + \lambda \sum_{j = 1}^p \beta_j^2.
\end{align}
We may rewrite this as
\begin{align}
    \gls{gammahat}^{\ridge} = \argmin_{\bgamma} ||\y - \U \gls{gamma}||^2_2 + \lambda \sum_{j = 1}^p \frac{\gamma_j^2}{d_j^2}. \label{RgPCR_ridge}
\end{align}
where $\bgamma = \D \V^T \bbeta$ and $d_j$ is the $j^{\text{th}}$ diagonal entry of $\D$. Hence, ridge regression can be viewed as a weighted $L_2$ penalized regression in the space of principal components. The RgPCR generalization comes from replacing the $L_2$ penalization with an arbitrary non-decreasing function $\gls{penalty}(\cdot)$. Some examples of these are LASSO, SCAD and MCP. In general, we write this RgPCR solution as

\begin{align}
    \hat{\bgamma} = \argmin_{\bgamma} ||\y - \U \bgamma||^2_2 + \sum_{j = 1}^p p_\lambda \left ( \frac{\gamma_j}{d_j} \right ), \label{RgPCR_def}
\end{align}

There are three main advantages to (\ref{RgPCR_def}). First, the regularization helps prevent overfitting. Secondly, the use of PCA and the orthogonality of $U$ allows for this minimizer to be calculated component-wise. Lastly, the penalty allows the variable selection to be determined by large variance in the predictors and the association with the response of principal components (we can see this by the presence of $\hat{\gamma}^{\text{ols}}_j$ in the following). 

The orthogonality of $\U$ allows us to solve (\ref{RgPCR_def}) component-wise. In particular,

\begin{align}
    \hat{\gamma}_j = \argmin_{\gamma_j}  \left ( \hat{\gamma}^{\text{ols}}_j - \gamma_j \right )^2 + p_\lambda \left ( \frac{\gamma_j}{d_j} \right ).  \label{RgPCR_def_j}
\end{align}

where $\hat{\gamma}^{\text{ols}}_j = \y^T U_j$ (a calculation can be found in \cite{langzou}). When $p_{\lambda}(t) = |t|$, the LASSO penalty, then we can find a closed form of (\ref{RgPCR_def_j}),

\begin{align}
	\hat{\gamma}_j^{\lasso} = \left ( |\gamma_j^{\ols}| - \frac{\lambda}{2d_j} \right )^+ \cdot \sgn{\hat{\gamma_j}^{\ols}}.
\end{align}

where $a^+$ denotes the positive real part of a real number $a$. We can recover the solution in the original coordinates by

\begin{align}
	\hat{\bbeta} = \sum_{j = 1}^p \tilde{V}_j \frac{\hat{\gamma}_j}{d_j}
\end{align}

%
%
%
%
%
%
%
%
%
%
%

\subsection{Logistic Regression}
Logistic regression is a common technique for classification. For now we consider binary classification. Let $\X$ be an $N \times p$ design matrix with $\y$ the $N \times 1$ classifications. Then if we assume that
\begin{align}
    \log \left ( \dfrac{Pr(G = 1 | X = x)}{Pr(G = 0|X = x)}\right ) = \bbeta^Tx \label{log-odds}
\end{align}
the log-odds are linear, then we may calculate the conditional probability
\begin{align}
    Pr(G = 1 | X = x) = \dfrac{\text{exp}(\bbeta^Tx)}{1 + \text{exp}(\bbeta^Tx)}.
\end{align}
Note that we may compute all probabilities for the data simultaneously by

\begin{align}
	\p = \dfrac{\text{exp}(\X \bbeta)}{1 + \text{exp}(\X \bbeta)}.
\end{align}

where $p_i$ is the probability that $x_i$ is in class 1.

To fit such a line for the log-odds (\ref{log-odds}), we fit by maximizing the log-likelihood,
\begin{align}
	\gls{loglike}(\X, \beta) = \sum_{i = 1}^N \left [ y_i \log (p(x_i; \bbeta)) + (1-y_i) \log(1 - p(x_i; \bbeta))\right ] \label{loglikelihood}
\end{align}
This calculation can be found in \citep{ESL} and has a convenient interpretation. When maximizing by the Newton-Ralphson method, each iteration can be interpreted as solving a weighted least squares problem. That is
\begin{align}
    \bbeta^{\new} &= \bbeta^{\old} + (\X^T \gls{W} \X)^{-1} \X^T(\y - \p) \\
    &= (\X^T \W \X)^{-1} \X^T \W \z
\end{align}
where
\begin{align}
	\p &= \dfrac{\text{exp}(\X \bbeta^{\old})}{1 + \text{exp}(\X \bbeta^{\old})} \label{pdef}\\
    \W &= \text{diag}[p_i(1-p_i)] \label{Wdef}\\
    \z &= \X \bbeta^{\old} + \W^{-1}(\y - \p). \label{zdef}
\end{align}
In particular, $\bbeta^{\new}$ is the solution to a weighted least squares problem with response $\z$, design $\X$ and weights $\W$.

For our purposes, it will be more convenient to state this as the following equivalent problem,
\begin{align}
    \bbeta^{\new} =  \argmin_{\bbeta} ||\W^{1/2}(\z - \X \bbeta)||^2
\end{align}
\end{document}